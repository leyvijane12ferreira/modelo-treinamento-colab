{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38b1419862cc42db9a18204d58ca3e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02049f1c46f04d53977484cb59484bdb",
              "IPY_MODEL_d0bf02ec9a2c46848edda68ed3d8abb1",
              "IPY_MODEL_98b0c11f232e475887ab7619a12aed0d"
            ],
            "layout": "IPY_MODEL_f6e242a25a0f4792a7d64e3c25aea26e"
          }
        },
        "02049f1c46f04d53977484cb59484bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e52e197e87794a34bd6a37395b7d371e",
            "placeholder": "​",
            "style": "IPY_MODEL_2de49e3a92e74bd58ac05a807b14f4f9",
            "value": "model.safetensors: 100%"
          }
        },
        "d0bf02ec9a2c46848edda68ed3d8abb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d2dc2b8e678423eb9877651bf1c385e",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8378affd21414ed09bccbafa6ba6af04",
            "value": 49335454
          }
        },
        "98b0c11f232e475887ab7619a12aed0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ec75684a0341c68944616ca75874d6",
            "placeholder": "​",
            "style": "IPY_MODEL_6d36316cd71b451eb35b8a4d01e29627",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 101MB/s]"
          }
        },
        "f6e242a25a0f4792a7d64e3c25aea26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e52e197e87794a34bd6a37395b7d371e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2de49e3a92e74bd58ac05a807b14f4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d2dc2b8e678423eb9877651bf1c385e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8378affd21414ed09bccbafa6ba6af04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37ec75684a0341c68944616ca75874d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d36316cd71b451eb35b8a4d01e29627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Dependência usada para o treinamento de modelos de deep learning, manipulação de tensores, GPU (CUDA)\n",
        "!pip install torch torchvision timm tqdm pillow"
      ],
      "metadata": {
        "id": "msQ-vz4wcA8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e25cb3a-3ef4-4fe5-d70d-231fbd6ad82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.16)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.33.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from timm import create_model\n",
        "from PIL import Image\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import drive   #Importar o módulo do Google Colab drive\n",
        "\n",
        "# --- Configuração Específica do Colab ---\n",
        "print(\"Montando Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive montado com sucesso.\")\n",
        "\n",
        "# Defina o caminho para o seu conjunto de dados compactado no Google Drive\n",
        "# IMPORTANTE: Ajuste este caminho se a sua estrutura de pastas for diferente\n",
        "zipped_dataset_path = '/content/drive/MyDrive/Dataset_librasMono/libras_CNN.zip'\n",
        "\n",
        "# Defina o caminho de extração dentro do ambiente Colab\n",
        "extraction_base_path = '/content/libras_CNN_extracted/'\n",
        "\n",
        "# Crie o diretório de extração se ele não existir\n",
        "os.makedirs(extraction_base_path, exist_ok=True)\n",
        "\n",
        "# Descompacte o conjunto de dados\n",
        "print(f\"Descompactando '{zipped_dataset_path}' para '{extraction_base_path}'...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(zipped_dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_base_path)\n",
        "    print(\"Dataset descompactado com sucesso!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo '{zipped_dataset_path}' não foi encontrado.\")\n",
        "    print(\"Por favor, verifique o caminho para o seu dataset compactado no Google Drive.\")\n",
        "    exit() # Saia se o conjunto de dados não for encontrado\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro durante a descompactação: {e}\")\n",
        "    exit()  # Saia em outros erros de descompactação\n",
        "\n",
        "# Verifique os arquivos extraídos (Opcional, mas altamente recomendado para depuração)\n",
        "print(\"\\nConteúdo do dataset descompactado (primeiros itens):\")\n",
        "# Você pode precisar ajustar isso dependendo da estrutura exata dentro do seu zip\n",
        "# Por exemplo, se libras_CNN.zip contém diretamente as pastas 'train' e 'test':\n",
        "# list_dir_path = extraction_base_path\n",
        "# Se libras_CNN.zip contém uma pasta como 'libras_CNN/' que por sua vez contém 'train' e 'test':\n",
        "list_dir_path = os.path.join(extraction_base_path, 'libras_CNN') # Cenário comum\n",
        "if os.path.exists(list_dir_path):\n",
        "    for root, dirs, files in os.walk(list_dir_path):\n",
        "        level = root.replace(list_dir_path, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print(f'{indent}{os.path.basename(root)}/')\n",
        "        if level < 2: # Apenas imprima os primeiros arquivos para evitar uma saída excessiva.\n",
        "            for f in files[:5]: # Mostre apenas até 5 arquivos por pasta\n",
        "                print(f'{indent}    {f}')\n",
        "            if len(files) > 5:\n",
        "                print(f'{indent}    ... ({len(files) - 5} more files)')\n",
        "        if level > 2 and not dirs and not files: # Pare mais fundo se não houver conteúdo\n",
        "            break\n",
        "else:\n",
        "    print(f\"Aviso: O caminho de verificação '{list_dir_path}' não foi encontrado. Verifique a estrutura do seu zip.\")\n",
        "\n",
        "\n",
        "# --- Adaptação de Código Original ---\n",
        "\n",
        "def load_data(data_path, img_size, mode='train'):\n",
        "    # Verificar o modo (treino ou teste) e aplicar as transformações adequadas\n",
        "    if mode == 'train':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    elif mode == 'test':\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        raise ValueError(\"O parâmetro 'mode' deve ser 'train' ou 'test'.\")\n",
        "\n",
        "    # Carregar o dataset com o caminho e as transformações apropriadas\n",
        "    dataset = datasets.ImageFolder(data_path, transform=transform)\n",
        "\n",
        "    # Retornar o dataset carregado e o número de classes\n",
        "    num_classes = len(dataset.classes)\n",
        "\n",
        "    return dataset, num_classes\n",
        "\n",
        "def split_dataset(dataset, val_ratio=0.2):\n",
        "    val_size = int(len(dataset) * val_ratio)\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def define_model(model_name, num_classes):\n",
        "    model = create_model(model_name, pretrained=True, num_classes=num_classes) # 192\n",
        "    return model\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, device, epochs=10, lr=5e-5):\n",
        "    print('\\n- - - - - Treinando o modelo - - - - -')\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    best_val_acc = 0.0  # Melhor acurácia de validação\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Treinamento\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Validação\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                with autocast():\n",
        "                    outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = 100 * correct / len(val_loader.dataset)\n",
        "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        # Salvar o modelo se for o melhor até agora\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            #print(f\"Melhor modelo salvo com acurácia de validação: {best_val_acc:.2f}%\")\n",
        "\n",
        "    #print(f\"Melhor Acurácia de Validação: {best_val_acc:.2f}%\")\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    #print('- - - - - Avaliando o modelo no conjunto de teste - - - - -')\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = 100 * correct / len(test_loader.dataset)\n",
        "    print(f\"Acurácia no Conjunto de Teste: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "def main():\n",
        "    IMG_SIZE = (64, 64)\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    # --- CAMINHOS AJUSTADOS PARA O COLAB ---\n",
        "    # Assumindo que o arquivo zip seja extraído para uma pasta chamada 'libras_CNN'\n",
        "    # que então contém subpastas 'treinar' e 'testar'.\n",
        "    # Ajuste 'libras_CNN' se a pasta raiz extraída tiver um nome diferente.\n",
        "    # Corrigindo os caminhos para usar o extraction_base_path\n",
        "    # TRAIN_PATH = os.path.join(extraction_base_path, 'libras_CNN', 'train')\n",
        "    # TEST_PATH = os.path.join(extraction_base_path, 'libras_CNN', 'test')\n",
        "    # Se o zip contiver diretamente as pastas 'train' e 'test', use:\n",
        "\n",
        "    # Caminhos corrigidos com base na saída da listagem do diretório\n",
        "    TRAIN_PATH = os.path.join(extraction_base_path, 'train')\n",
        "    TEST_PATH = os.path.join(extraction_base_path, 'test')\n",
        "    # Você pode precisar verificar a estrutura real de pastas após descompactar\n",
        "\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "    train_dataset, num_classes = load_data(TRAIN_PATH, IMG_SIZE, 'train')\n",
        "\n",
        "    test_dataset, _ = load_data(TEST_PATH, IMG_SIZE, 'test')\n",
        "\n",
        "    train_dataset, val_dataset = split_dataset(train_dataset)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    print('- - - - - - - - - - - - - - - - - - - - -')\n",
        "    print(f\"Caminho do treino: {TRAIN_PATH}\")\n",
        "    print(f\"Caminho do teste: {TEST_PATH}\")\n",
        "    print(f\"Número de classes: {num_classes}\")\n",
        "    print(f\"Quantidade de imagens de treino: {len(train_dataset)}\")\n",
        "    print(f\"Quantidade de imagens de validação: {len(val_dataset)}\")\n",
        "    print(f\"Quantidade de imagens de teste: {len(test_dataset)}\")\n",
        "    print('- - - - - - - - - - - - - - - - - - - - -')\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Dispositivo usado: {device}\")\n",
        "\n",
        "    # Esta linha já está configurada para o modelo 'efficientnet_b3'\n",
        "    model_names = ['efficientnet_b3']\n",
        "\n",
        "\n",
        "    accuracies_list1 = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        print(f\"\\n--- Iniciando treinamento para o modelo: {model_name} ---\")\n",
        "        model = define_model(model_name, num_classes)\n",
        "        train_and_validate(model, train_loader, val_loader, device, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "        print(f\"Treino usado: {TRAIN_PATH}\")\n",
        "        print('\\n- - - - - - - Avaliando o modelo no conjunto de teste - - - - - - -')\n",
        "        acc1 = evaluate_model(model, test_loader, device)\n",
        "        accuracies_list1.append(acc1)\n",
        "\n",
        "    print(f\"\\nAcurácias finais dos modelos no conjunto de teste:\")\n",
        "    for model_name, accuracy in zip(model_names, accuracies_list1):\n",
        "        print(f\"{model_name}: {accuracy:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "38b1419862cc42db9a18204d58ca3e46",
            "02049f1c46f04d53977484cb59484bdb",
            "d0bf02ec9a2c46848edda68ed3d8abb1",
            "98b0c11f232e475887ab7619a12aed0d",
            "f6e242a25a0f4792a7d64e3c25aea26e",
            "e52e197e87794a34bd6a37395b7d371e",
            "2de49e3a92e74bd58ac05a807b14f4f9",
            "1d2dc2b8e678423eb9877651bf1c385e",
            "8378affd21414ed09bccbafa6ba6af04",
            "37ec75684a0341c68944616ca75874d6",
            "6d36316cd71b451eb35b8a4d01e29627"
          ]
        },
        "id": "NWejx8lPcTmc",
        "outputId": "e7bc83e5-a020-4064-d7e0-240e3dcad0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Montando Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive montado com sucesso.\n",
            "Descompactando '/content/drive/MyDrive/Dataset_librasMono/libras_CNN.zip' para '/content/libras_CNN_extracted/'...\n",
            "Dataset descompactado com sucesso!\n",
            "\n",
            "Conteúdo do dataset descompactado (primeiros itens):\n",
            "Aviso: O caminho de verificação '/content/libras_CNN_extracted/libras_CNN' não foi encontrado. Verifique a estrutura do seu zip.\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "Caminho do treino: /content/libras_CNN_extracted/train\n",
            "Caminho do teste: /content/libras_CNN_extracted/test\n",
            "Número de classes: 21\n",
            "Quantidade de imagens de treino: 27772\n",
            "Quantidade de imagens de validação: 6942\n",
            "Quantidade de imagens de teste: 11548\n",
            "- - - - - - - - - - - - - - - - - - - - -\n",
            "Dispositivo usado: cuda\n",
            "\n",
            "--- Iniciando treinamento para o modelo: efficientnet_b3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38b1419862cc42db9a18204d58ca3e46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- - - - - Treinando o modelo - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-2199186293.py:114: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "Epoch 1/10:   0%|          | 0/868 [00:00<?, ?it/s]/tmp/ipython-input-2-2199186293.py:126: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Epoch 1/10: 100%|██████████| 868/868 [01:44<00:00,  8.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/tmp/ipython-input-2-2199186293.py:143: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 868/868 [01:42<00:00,  8.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.0725\n",
            "Validation Accuracy: 99.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 868/868 [01:44<00:00,  8.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.0343\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 868/868 [01:42<00:00,  8.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.0508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 868/868 [01:43<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 868/868 [01:42<00:00,  8.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 868/868 [01:44<00:00,  8.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0385\n",
            "Validation Accuracy: 99.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 868/868 [01:42<00:00,  8.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 868/868 [01:43<00:00,  8.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 868/868 [01:41<00:00,  8.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.35%\n",
            "Treino usado: /content/libras_CNN_extracted/train\n",
            "\n",
            "- - - - - - - Avaliando o modelo no conjunto de teste - - - - - - -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 361/361 [00:12<00:00, 27.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acurácia no Conjunto de Teste: 98.46%\n",
            "\n",
            "Acurácias finais dos modelos no conjunto de teste:\n",
            "efficientnet_b3: 98.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07de7a8d",
        "outputId": "d15803d5-269e-4fe4-9ee8-e3e52528e936"
      },
      "source": [
        "import os\n",
        "\n",
        "extraction_base_path = '/content/libras_CNN_extracted/'\n",
        "\n",
        "print(f\"Conteúdo de '{extraction_base_path}':\")\n",
        "# Listar todos os itens no diretório de extração\n",
        "for item in os.listdir(extraction_base_path):\n",
        "    print(item)\n",
        "\n",
        "# Se houver uma subpasta, você pode querer listar seu conteúdo também.\n",
        "# Por exemplo, se você suspeitar que há uma pasta chamada 'libras_CNN' dentro dela:\n",
        "# subfolder_path = os.path.join(extraction_base_path, 'libras_CNN')\n",
        "# if os.path.exists(subfolder_path):\n",
        "# print(f\"\\nConteúdo de '{subfolder_path}':\")\n",
        "# for item in os.listdir(subfolder_path):\n",
        "# print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conteúdo de '/content/libras_CNN_extracted/':\n",
            "train\n",
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instala as bibliotecas que você vai precisar para a conversão\n",
        "!pip install onnx onnxruntime onnx-tf tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRvk_Xa-edHB",
        "outputId": "2adfbf17-3464-41a9-8b42-f7cbee3c561d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.10.0-py3-none-any.whl.metadata (510 bytes)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from onnx-tf) (6.0.2)\n",
            "Collecting tensorflow-addons (from onnx-tf)\n",
            "  Downloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons->onnx-tf)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, onnx, humanfriendly, tensorflow-addons, coloredlogs, onnxruntime, onnx-tf\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.4\n",
            "    Uninstalling typeguard-4.4.4:\n",
            "      Successfully uninstalled typeguard-4.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.18.0 onnx-tf-1.10.0 onnxruntime-1.22.0 tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Carregar o Modelo .pth e Exportar para ONNX"
      ],
      "metadata": {
        "id": "CAKZ3T_Vex9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from timm import create_model # Garanta que 'timm' está instalado e disponível\n",
        "\n",
        "# --- Definir Parâmetros Chave ---\n",
        "# Esses valores DEVEM ser os mesmos que você usou no TREINAMENTO do seu modelo.\n",
        "# Se você mudou o IMG_SIZE para 224x224 durante o treinamento, use 224 aqui.\n",
        "IMG_SIZE = (64, 64) # Adapte para o IMG_SIZE real do seu treinamento\n",
        "# O número de classes é essencial. Você pode obtê-lo da sua função 'load_data'\n",
        "# ou simplesmente do número de subpastas (classes) no seu dataset de treinamento.\n",
        "# Se a sua última execução de 'main()' imprimiu 'Número de classes: X', use esse X.\n",
        "# Exemplo: NUM_CLASSES = 192\n",
        "NUM_CLASSES = 192 # <--- ATUALIZE ESTE VALOR COM O NÚMERO REAL DE CLASSES DO SEU DATASET!\n",
        "\n",
        "MODEL_NAME = 'efficientnet_b3' # O nome do modelo que você treinou\n",
        "\n",
        "# --- Carregar o Modelo Treinado do .pth ---\n",
        "print(f\"\\nCarregando modelo '{MODEL_NAME}' de 'best_model.pth' para exportação...\")\n",
        "# 1. Crie uma nova instância do modelo com a mesma arquitetura usada no treinamento.\n",
        "# 'pretrained=False' porque você vai carregar seus próprios pesos, não os pré-treinados do ImageNet.\n",
        "model_to_export = create_model(MODEL_NAME, pretrained=False, num_classes=NUM_CLASSES)\n",
        "\n",
        "# 2. Carregue os pesos que você salvou no arquivo 'best_model.pth'.\n",
        "# 'map_location=torch.device('cpu')' garante que funcione mesmo se você estiver em CPU ou GPU.\n",
        "try:\n",
        "    model_to_export.load_state_dict(torch.load('best_model.pth', map_location=torch.device('cpu')))\n",
        "    print(\"Pesos do modelo 'best_model.pth' carregados com sucesso!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERRO: 'best_model.pth' não encontrado. Certifique-se de que seu treinamento foi concluído e o arquivo foi salvo no diretório correto do Colab.\")\n",
        "    print(\"Você pode verificar clicando no ícone de pasta à esquerda no Colab.\")\n",
        "    exit() # Interrompe a execução se o arquivo não for encontrado\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao carregar os pesos do modelo: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 3. Coloque o modelo em modo de avaliação. Isso desativa camadas como Dropout e Batch Normalization,\n",
        "# que são usadas apenas durante o treinamento.\n",
        "model_to_export.eval()\n",
        "\n",
        "# --- Exportar para ONNX ---\n",
        "onnx_model_filename = 'model_efficientnet_b3.onnx' # Nome do arquivo ONNX de saída\n",
        "\n",
        "# 1. Crie um tensor de entrada de exemplo (dummy input).\n",
        "# Ele simula uma única imagem RGB (3 canais) com as dimensões esperadas pelo seu modelo (IMG_SIZE).\n",
        "# Formato: (batch_size, channels, height, width)\n",
        "dummy_input = torch.randn(1, 3, IMG_SIZE[0], IMG_SIZE[1])\n",
        "print(f\"Criado tensor de entrada de exemplo com shape: {dummy_input.shape}\")\n",
        "\n",
        "# 2. Realize a exportação para ONNX.\n",
        "print(f\"Iniciando exportação do modelo para ONNX em: {onnx_model_filename}...\")\n",
        "try:\n",
        "    torch.onnx.export(model_to_export,\n",
        "                      dummy_input, # Entrada de exemplo\n",
        "                      onnx_model_filename, # Nome do arquivo de saída\n",
        "                      verbose=False, # Defina como True para ver mais detalhes da exportação (útil para debug)\n",
        "                      input_names=['input'], # Nomeia a entrada do modelo\n",
        "                      output_names=['output'], # Nomeia a saída do modelo\n",
        "                      # Permite que o tamanho do batch seja flexível (útil para inferência)\n",
        "                      dynamic_axes={'input': {0: 'batch_size'},\n",
        "                                    'output': {0: 'batch_size'}})\n",
        "    print(\"Modelo exportado para ONNX com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao exportar para ONNX: {e}\")\n",
        "    print(\"Verifique se as dependências ONNX estão corretas e se o modelo não tem operações não suportadas.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoH_Ds_VeqZt",
        "outputId": "2fd8f0fc-24ee-432f-f2a5-c930a9614c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregando modelo 'efficientnet_b3' de 'best_model.pth' para exportação...\n",
            "ERRO ao carregar os pesos do modelo: Error(s) in loading state_dict for EfficientNet:\n",
            "\tsize mismatch for classifier.weight: copying a param with shape torch.Size([21, 1536]) from checkpoint, the shape in current model is torch.Size([192, 1536]).\n",
            "\tsize mismatch for classifier.bias: copying a param with shape torch.Size([21]) from checkpoint, the shape in current model is torch.Size([192]).\n",
            "Criado tensor de entrada de exemplo com shape: torch.Size([1, 3, 64, 64])\n",
            "Iniciando exportação do modelo para ONNX em: model_efficientnet_b3.onnx...\n",
            "Modelo exportado para ONNX com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall tensorflow-addons -y\n",
        "!pip uninstall onnx-tf -y # Pode ser útil reinstalar também"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV_e_SGvgrx-",
        "outputId": "2d3b2c42-a42a-4a71-b854-0b02008d2ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "Found existing installation: tensorflow-addons 0.23.0\n",
            "Uninstalling tensorflow-addons-0.23.0:\n",
            "  Successfully uninstalled tensorflow-addons-0.23.0\n",
            "Found existing installation: onnx-tf 1.10.0\n",
            "Uninstalling onnx-tf-1.10.0:\n",
            "  Successfully uninstalled onnx-tf-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CÉLULA 1: Instalação de Dependências e Reinício ---\n",
        "print(\"Desinstalando bibliotecas existentes...\")\n",
        "!pip uninstall tensorflow -y\n",
        "!pip uninstall tensorflow-addons -y\n",
        "!pip uninstall onnx-tf -y\n",
        "!pip uninstall tensorflow-probability -y\n",
        "!pip uninstall keras -y\n",
        "\n",
        "print(\"\\nInstalando versões compatíveis...\")\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install tensorflow-addons==0.23.0\n",
        "!pip install onnx-tf==1.10.0\n",
        "!pip install onnx onnxruntime tensorflow-probability==0.23.0\n",
        "\n",
        "print(\"\\nINSTALAÇÃO CONCLUÍDA. POR FAVOR, REINICIE O AMBIENTE DE EXECUÇÃO (Runtime -> Restart runtime...) AGORA!\")\n",
        "#return # Mantenha este 'return' AQUI para forçar o reinício manual."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xOlIR2dOkKW6",
        "outputId": "6676530f-a16c-4072-b310-7fc3d285783d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Desinstalando bibliotecas existentes...\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-addons as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping onnx-tf as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-probability 0.25.0\n",
            "Uninstalling tensorflow-probability-0.25.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.25.0\n",
            "Found existing installation: keras 3.8.0\n",
            "Uninstalling keras-3.8.0:\n",
            "  Successfully uninstalled keras-3.8.0\n",
            "\n",
            "Instalando versões compatíveis...\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\n",
            "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow-probability>=0.13.0, which is not installed.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 numpy-1.26.4 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              },
              "id": "4717628d53454656b255d57652f4f1b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.23.0\n",
            "  Using cached tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.23.0) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.23.0) (2.13.3)\n",
            "Using cached tensorflow_addons-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0\n",
            "Collecting onnx-tf==1.10.0\n",
            "  Using cached onnx_tf-1.10.0-py3-none-any.whl.metadata (510 bytes)\n",
            "Requirement already satisfied: onnx>=1.10.2 in /usr/local/lib/python3.11/dist-packages (from onnx-tf==1.10.0) (1.18.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from onnx-tf==1.10.0) (6.0.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.11/dist-packages (from onnx-tf==1.10.0) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (4.25.8)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.10.2->onnx-tf==1.10.0) (4.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons->onnx-tf==1.10.0) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons->onnx-tf==1.10.0) (2.13.3)\n",
            "Using cached onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n",
            "Installing collected packages: onnx-tf\n",
            "Successfully installed onnx-tf-1.10.0\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
            "Collecting tensorflow-probability==0.23.0\n",
            "  Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (1.26.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability==0.23.0) (0.1.9)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.8)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.23.0) (25.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability==0.23.0) (1.14.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading tensorflow_probability-0.23.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow-probability\n",
            "Successfully installed tensorflow-probability-0.23.0\n",
            "\n",
            "INSTALAÇÃO CONCLUÍDA. POR FAVOR, REINICIE O AMBIENTE DE EXECUÇÃO (Runtime -> Restart runtime...) AGORA!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Converter ONNX para TensorFlow Lite (TFLite)"
      ],
      "metadata": {
        "id": "27Uy7DSnfJmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CÉLULA 2: Conversão ONNX para TFLite (APÓS O REINÍCIO) ---\n",
        "import onnx\n",
        "from onnx_tf.backend import prepare\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil # Usado para remover o diretório temporário\n",
        "\n",
        "# --- Definir Parâmetros Chave (os mesmos que no treinamento e exportação ONNX) ---\n",
        "# Adapte para o IMG_SIZE real do seu treinamento (ex: (224, 224) se você mudou)\n",
        "IMG_SIZE = (64, 64)\n",
        "\n",
        "# --- Definir Caminhos de Arquivos ---\n",
        "# Nome do arquivo ONNX gerado no Passo 1 (certifique-se que ele existe na raiz do Colab)\n",
        "onnx_model_filename = 'model_efficientnet_b3.onnx'\n",
        "# Nome do arquivo TFLite de saída\n",
        "tflite_model_filename = 'model_efficientnet_b3.tflite'\n",
        "# Novo diretório temporário\n",
        "tf_model_temp_dir = 'tf_model_temp_dir_v2'\n",
        "\n",
        "# --- Verificar se o arquivo ONNX existe ---\n",
        "if not os.path.exists(onnx_model_filename):\n",
        "    print(f\"ERRO: O arquivo '{onnx_model_filename}' não foi encontrado. Certifique-se de que o Passo 1 foi executado com sucesso e o arquivo foi gerado.\")\n",
        "    exit() # Permanece aqui para parar se o ONNX não existir\n",
        "\n",
        "print(f\"\\nCarregando modelo ONNX de: {onnx_model_filename}...\")\n",
        "try:\n",
        "    onnx_model = onnx.load(onnx_model_filename)\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"Modelo ONNX carregado e validado com sucesso.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao carregar/validar modelo ONNX: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- Converter ONNX para TensorFlow SavedModel ---\n",
        "print(\"\\nConvertendo ONNX para o formato TensorFlow SavedModel...\")\n",
        "try:\n",
        "    tf_rep = prepare(onnx_model)\n",
        "    tf_rep.export_graph(tf_model_temp_dir)\n",
        "    print(f\"Modelo TensorFlow SavedModel salvo temporariamente em: {tf_model_temp_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao converter ONNX para TensorFlow SavedModel: {e}\")\n",
        "    if os.path.exists(tf_model_temp_dir):\n",
        "        shutil.rmtree(tf_model_temp_dir)\n",
        "    exit()\n",
        "\n",
        "# --- Converter TensorFlow SavedModel para TensorFlow Lite ---\n",
        "print(\"\\nConvertendo TensorFlow SavedModel para TensorFlow Lite (.tflite)...\")\n",
        "try:\n",
        "    converter = tf.lite.TFLiteConverter.from_saved_model(tf_model_temp_dir)\n",
        "\n",
        "    # Otimizações Opcionais (recomendado)\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    with open(tflite_model_filename, 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "    print(f\"Modelo TFLite salvo com sucesso em: {tflite_model_filename}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao converter TensorFlow SavedModel para TFLite: {e}\")\n",
        "finally:\n",
        "    if os.path.exists(tf_model_temp_dir):\n",
        "        shutil.rmtree(tf_model_temp_dir)\n",
        "        print(f\"Diretório temporário '{tf_model_temp_dir}' removido.\")\n",
        "\n",
        "# --- Baixar o Modelo TFLite ---\n",
        "from google.colab import files\n",
        "try:\n",
        "    files.download(tflite_model_filename)\n",
        "    print(f\"\\nArquivo '{tflite_model_filename}' baixado com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao baixar o arquivo: {e}\")\n",
        "    print(f\"Verifique se '{tflite_model_filename}' existe no diretório.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "QN9GzzmmfJDK",
        "outputId": "2d5b973f-c19a-4bd7-a689-6fde228d9ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Carregando modelo ONNX de: model_efficientnet_b3.onnx...\n",
            "Modelo ONNX carregado e validado com sucesso.\n",
            "\n",
            "Convertendo ONNX para o formato TensorFlow SavedModel...\n",
            "Modelo TensorFlow SavedModel salvo temporariamente em: tf_model_temp_dir_v2\n",
            "\n",
            "Convertendo TensorFlow SavedModel para TensorFlow Lite (.tflite)...\n",
            "Modelo TFLite salvo com sucesso em: model_efficientnet_b3.tflite\n",
            "Diretório temporário 'tf_model_temp_dir_v2' removido.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6b70f1e8-5cd4-4111-86c3-d71408c082da\", \"model_efficientnet_b3.tflite\", 12051336)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Arquivo 'model_efficientnet_b3.tflite' baixado com sucesso!\n"
          ]
        }
      ]
    }
  ]
}